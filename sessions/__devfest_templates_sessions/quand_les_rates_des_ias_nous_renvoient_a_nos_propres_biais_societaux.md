---
key: quand_les_rates_des_ias_nous_renvoient_a_nos_propres_biais_societaux
title: Quand les ratés des IAs nous renvoient à nos propres biais sociétaux
id: ZsAGxn6UZi1hfzz2bwR7
format: conference
tags:
  - _big_data___ml___ai
level: beginner
speakers:
  - clement_duffau
  - melanie_ducoffe
videoId: gB5qsODoGhU
presentation: null
draft: false
---
Un chatbot créé par Microsoft qui dérive avec des propos antisémites, Amazon qui scanne des CVs et finit par ne recruter que des hommes blancs, des algorithmes de reconnaissance faciale qui ont du mal avec les personnes de couleur noire, ... Que d'exemples où l'IA a intégré nos propres biais sociétaux. Il est urgent que nous, développeurs, prenions nos responsabilités et mesurions les enjeux éthiques de l'IA pour éviter que les stéréotypes, les inégalités et les préjugés se retrouvent au cœur de nos futurs systèmes.

Cette présentation propose des root cause analysis sur des exemples concrets de ces biais et présente des alternatives qui auraient permis d'éviter ces biais en "production" autant d'un point de vue humain que technique.

Nous verrons notamment que l’éthique peut directement être abordée au niveau des données et de leur préparation à l’apprentissage. Nous nous concentrerons principalement sur l’impact des statistiques des données d’entraînement et les transformations à appliquer en pré-processing (métriques de fairness, améliorer la fairness sur des données annotées), et nous montrerons comment des benchmarks permettent d'appréhender l’impact des méthodes de fairness sur la précision des modèles entraînés.

La confiance et l’équité passe avant tout par une compréhension de la prise de décision par l’utilisateur. Par conséquent, nous présenterons diverses méthodes pour expliquer la prise de décision d’un modèle boîte noire (principalement des algorithmes de deep learning) et mettons en garde contre une mauvaise interprétation de ces explications.
